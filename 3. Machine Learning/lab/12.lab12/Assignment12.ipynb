{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNF8JBGu72cC"
      },
      "source": [
        "#1. Implement the MLP (own code) using the IRIS dataset and report the class-wise and overall accuracy for 5-fold cross validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhnpCUTtj_Xm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "from statistics import mean\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d01qdki6dcg5",
        "outputId": "f5884e89-0e1c-43d6-e3a3-5a40e50c476b"
      },
      "source": [
        "#loading the data\n",
        "wine=datasets.load_wine()\n",
        "print(wine.DESCR)\n",
        "df=pd.DataFrame(data=np.c_[wine['data'],wine['target']],columns=wine['feature_names']+['target'])\n",
        "X=pd.DataFrame(wine.data)\n",
        "y=pd.DataFrame(wine.target)\n",
        "print(\"ORIGINAL DATA:\")\n",
        "print(df)\n",
        "\n",
        "#normalize the dataset\n",
        "for column in X.columns:\n",
        "    X[column] = (X[column] - X[column].min()) / (X[column].max() - X[column].min()) \n",
        "\n",
        "\n",
        "df=X.copy()\n",
        "df['target']=y\n",
        "\n",
        "#DATA PREPROCESSING- ONE HOT ENCODING\n",
        "y = pd.get_dummies(df.target, prefix='target')\n",
        "y = LabelBinarizer().fit_transform(df.target)\n",
        "X=X.to_numpy()\n",
        "X, y = shuffle(X, y)\n",
        "\n",
        "print(\"After one hot encoding: \\n\",X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 178 (50 in each of three classes)\n",
            "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            " \t\t- Alcohol\n",
            " \t\t- Malic acid\n",
            " \t\t- Ash\n",
            "\t\t- Alcalinity of ash  \n",
            " \t\t- Magnesium\n",
            "\t\t- Total phenols\n",
            " \t\t- Flavanoids\n",
            " \t\t- Nonflavanoid phenols\n",
            " \t\t- Proanthocyanins\n",
            "\t\t- Color intensity\n",
            " \t\t- Hue\n",
            " \t\t- OD280/OD315 of diluted wines\n",
            " \t\t- Proline\n",
            "\n",
            "    - class:\n",
            "            - class_0\n",
            "            - class_1\n",
            "            - class_2\n",
            "\t\t\n",
            "    :Summary Statistics:\n",
            "    \n",
            "    ============================= ==== ===== ======= =====\n",
            "                                   Min   Max   Mean     SD\n",
            "    ============================= ==== ===== ======= =====\n",
            "    Alcohol:                      11.0  14.8    13.0   0.8\n",
            "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "    Ash:                          1.36  3.23    2.36  0.27\n",
            "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "    Magnesium:                    70.0 162.0    99.7  14.3\n",
            "    Total Phenols:                0.98  3.88    2.29  0.63\n",
            "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "    Hue:                          0.48  1.71    0.96  0.23\n",
            "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "    Proline:                       278  1680     746   315\n",
            "    ============================= ==== ===== ======= =====\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners: \n",
            "\n",
            "Forina, M. et al, PARVUS - \n",
            "An Extendible Package for Data Exploration, Classification and Correlation. \n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science. \n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  Comparison of Classifiers in High Dimensional Settings, \n",
            "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Technometrics). \n",
            "\n",
            "  The data was used with many others for comparing various \n",
            "  classifiers. The classes are separable, though only RDA \n",
            "  has achieved 100% correct classification. \n",
            "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
            "  (All results using the leave-one-out technique) \n",
            "\n",
            "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
            "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Journal of Chemometrics).\n",
            "\n",
            "ORIGINAL DATA:\n",
            "     alcohol  malic_acid   ash  ...  od280/od315_of_diluted_wines  proline  target\n",
            "0      14.23        1.71  2.43  ...                          3.92   1065.0     0.0\n",
            "1      13.20        1.78  2.14  ...                          3.40   1050.0     0.0\n",
            "2      13.16        2.36  2.67  ...                          3.17   1185.0     0.0\n",
            "3      14.37        1.95  2.50  ...                          3.45   1480.0     0.0\n",
            "4      13.24        2.59  2.87  ...                          2.93    735.0     0.0\n",
            "..       ...         ...   ...  ...                           ...      ...     ...\n",
            "173    13.71        5.65  2.45  ...                          1.74    740.0     2.0\n",
            "174    13.40        3.91  2.48  ...                          1.56    750.0     2.0\n",
            "175    13.27        4.28  2.26  ...                          1.56    835.0     2.0\n",
            "176    13.17        2.59  2.37  ...                          1.62    840.0     2.0\n",
            "177    14.13        4.10  2.74  ...                          1.60    560.0     2.0\n",
            "\n",
            "[178 rows x 14 columns]\n",
            "After one hot encoding: \n",
            " [[0.35263158 0.06521739 0.39572193 ... 0.45528455 0.54945055 0.2724679 ]\n",
            " [0.21315789 0.02964427 0.65240642 ... 0.25203252 0.66300366 0.17261056]\n",
            " [0.43947368 0.55533597 0.53475936 ... 0.24390244 0.00732601 0.2296719 ]\n",
            " ...\n",
            " [0.27631579 0.21541502 0.51336898 ... 0.48780488 0.36630037 0.14407989]\n",
            " [0.69473684 0.10079051 0.29946524 ... 0.6097561  0.43589744 0.2510699 ]\n",
            " [0.25526316 0.53162055 0.34224599 ... 0.36585366 0.65201465 0.20399429]] [[0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT7KietldxJz"
      },
      "source": [
        "def MLP(X,y,learning_rate,epochs):\n",
        "  \n",
        "  X = np.insert(X, 0 , 1 , axis=1)\n",
        "  l=learning_rate\n",
        "  #weights for hidden layer\n",
        "  W_hidden=[]\n",
        "  for i in range(X.shape[1]):\n",
        "    w=[]\n",
        "    for j in range(X.shape[1]):\n",
        "       w.append(0.1)\n",
        "    W_hidden.append(w)   \n",
        " \n",
        "  W_hidden= np.array(W_hidden)  \n",
        "  \n",
        "  #weights for output layer\n",
        "  W_output=[]\n",
        "  for i in range(3):\n",
        "    w=[]\n",
        "    for j in range(X.shape[1]+1):\n",
        "       w.append(0.1)\n",
        "    W_output.append(w)   \n",
        " \n",
        "  W_output= np.array(W_output)  \n",
        "  visited_index=[]\n",
        "  index = random.randint(0, len(X)-1)\n",
        "  epoch=1\n",
        "  converged=True\n",
        "  err_in=0\n",
        "  err=0\n",
        "  while converged:\n",
        "\n",
        "    #input to hidden\n",
        "    h=1/(1+np.exp(-np.dot(W_hidden,np.transpose(X[index]))))\n",
        "    \n",
        "    h1 = np.insert(h, 0 , 1)\n",
        "    #hidden to output\n",
        "    d=1/(1+np.exp(-np.dot(W_output,np.transpose(h1))))\n",
        "    \n",
        "    err_in= err_in+(1/2)*sum((d-y[index])**2)\n",
        "    \n",
        "    #local gradient error in output neuron\n",
        "    grad_err_output=[]\n",
        "    for i in range(len(d)):\n",
        "      grad_err_output.append(d[i]*(1-d[i])*(y[index][i]-d[i]))\n",
        "\n",
        "\n",
        "    #local gradient error in hidden neuron\n",
        "    grad_err_hidden=[]\n",
        "    for i in range(len(h)):\n",
        "      grad_err_hidden.append(float(W_output[0][i]*grad_err_output[0] + W_output[1][i]*grad_err_output[1] + W_output[2][i]*grad_err_output[2])* (float)(h[i]) * (float)(1 - h[i]))\n",
        "\n",
        "     #update weights between hidden and output\n",
        "    for i in range(W_output.shape[0]):\n",
        "        for j in range(W_output.shape[1]):\n",
        "          W_output[i][j]=W_output[i][j]+(l*grad_err_output[i]*h1[j])  \n",
        "\n",
        "    #update weights between input and hidden\n",
        "    for i in range(W_hidden.shape[0]):\n",
        "        for j in range(W_hidden.shape[1]):\n",
        "          W_hidden[i][j]=W_hidden[i][j]+(l*grad_err_hidden[i]*X[index][j])      \n",
        "    \n",
        "    index = random.randint(0, len(X)-1)\n",
        "    visited_index.append(index)\n",
        "    \n",
        "    if (len(set(visited_index))==len(X)):\n",
        "      epoch=epoch+1\n",
        "      d=1/(1+np.exp(-np.dot(W_output,np.transpose(h1))))\n",
        "      err= err+(1/2)*sum((d-y[index])**2)\n",
        "    \n",
        "      if(abs(err-err_in)<0.001 or epoch==epochs):\n",
        "        print(\"Successfully converged\")\n",
        "        print(\"No of epochs: \",epoch)\n",
        "        converged=False\n",
        "      else:\n",
        "        visited_index=[]\n",
        "        err_in=0\n",
        "        err=0  \n",
        "\n",
        "  return W_hidden , W_output , err"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZbLNkmbd9jo"
      },
      "source": [
        "def predict(X, y ,w1, w2):\n",
        "  \n",
        "  X = np.insert(X, 0 , 1 , axis=1)\n",
        "  Y_predict=np.zeros(len(y))\n",
        "  Y=np.zeros(len(y))\n",
        "  h=[]\n",
        "  d=[]\n",
        "  for i in range(len(y)):\n",
        "     d1= 1/(1+np.exp(-(np.dot(w1,np.transpose(X[i])))))\n",
        "     h.append(d1)\n",
        "\n",
        "  for i in range(len(h)):\n",
        "     h1 = np.insert(h[i], 0 , 1)\n",
        "     d1= 1/(1+np.exp(-(np.dot(w2,np.transpose(h1)))))\n",
        "     d.append(d1)   \n",
        " \n",
        "  for i in range(len(d)):\n",
        "     max_index= np.argmax(d[i], axis=0)\n",
        "     Y_predict[i]=max_index\n",
        "\n",
        "  for i in range(len(y)):\n",
        "    max_index= np.argmax(y[i], axis=0)\n",
        "    Y[i]=max_index    \n",
        "\n",
        "  test_acc=accuracy_score(Y,Y_predict)  \n",
        "  \n",
        "  return test_acc,Y,Y_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SXC3CUMeKHO",
        "outputId": "784d5fdf-15b3-4918-a246-3a8bfc5711ba"
      },
      "source": [
        "#splitting the dataset\n",
        "\n",
        "# In the first step we will split the data in training and remaining dataset\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.6)\n",
        "\n",
        "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
        "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
        "test_size = 0.5\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
        "\n",
        "#Training the model\n",
        "w=MLP(X_train,y_train,0.2,70)\n",
        "print(\"Accuracy for the test set: \",predict(X_test,y_test,w[0],w[1])[0]*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converged\n",
            "No of epochs:  70\n",
            "Accuracy for the test set:  100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvebUSXteRba",
        "outputId": "0f9241f4-6ff5-4a79-f74f-61d6c6c5949b"
      },
      "source": [
        "k = 5\n",
        "kf = KFold(n_splits=k, random_state=None) \n",
        "acc_score = []\n",
        " \n",
        "for train_index , test_index in kf.split(X):\n",
        "    X_train , X_test = X[train_index,:],X[test_index,:]\n",
        "    y_train , y_test = y[train_index,:] , y[test_index,:]\n",
        "     \n",
        "    w=MLP(X_train,y_train,0.2,70)\n",
        "     \n",
        "    acc = predict(X_test , y_test,w[0],w[1])\n",
        "    acc_score.append(acc[0]*100)\n",
        "    confusion(acc[1],acc[2])\n",
        "     \n",
        "avg_acc_score = sum(acc_score)/k\n",
        " \n",
        "print('accuracy of each fold - {}'.format(acc_score))\n",
        "print('Avg accuracy : {}'.format(avg_acc_score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converged\n",
            "No of epochs:  70\n",
            "Class-wise Accuracy:  [1. 1. 1.]\n",
            "Class-wise Precision:  [1. 1. 1.]\n",
            "Class-wise Recall:  [1. 1. 1.]\n",
            "Successfully converged\n",
            "No of epochs:  70\n",
            "Class-wise Accuracy:  [0.91666667 1.         0.90909091]\n",
            "Class-wise Precision:  [1.         0.86666667 1.        ]\n",
            "Class-wise Recall:  [0.91666667 1.         0.90909091]\n",
            "Successfully converged\n",
            "No of epochs:  70\n",
            "Class-wise Accuracy:  [1.         0.85714286 1.        ]\n",
            "Class-wise Precision:  [0.94117647 1.         1.        ]\n",
            "Class-wise Recall:  [1.         0.85714286 1.        ]\n",
            "Successfully converged\n",
            "No of epochs:  70\n",
            "Class-wise Accuracy:  [1.         0.94117647 1.        ]\n",
            "Class-wise Precision:  [1.  1.  0.9]\n",
            "Class-wise Recall:  [1.         0.94117647 1.        ]\n",
            "Successfully converged\n",
            "No of epochs:  70\n",
            "Class-wise Accuracy:  [1. 1. 1.]\n",
            "Class-wise Precision:  [1. 1. 1.]\n",
            "Class-wise Recall:  [1. 1. 1.]\n",
            "accuracy of each fold - [100.0, 94.44444444444444, 97.22222222222221, 97.14285714285714, 100.0]\n",
            "Avg accuracy : 97.76190476190477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f22Wk93QebFu"
      },
      "source": [
        "def confusion(Y,Y_predict):\n",
        "\n",
        "  cm = confusion_matrix(Y, Y_predict) \n",
        "  p=precision_score(Y, Y_predict, average=None, zero_division=1)\n",
        "  r=recall_score(Y, Y_predict, average=None, zero_division=1)\n",
        "  \n",
        "  #Now the normalize the diagonal entries\n",
        "\n",
        "  cm = cm.astype('float') / cm.sum(axis=1) \n",
        "  print(\"Class-wise Accuracy: \",cm.diagonal())\n",
        "  print(\"Class-wise Precision: \",p)\n",
        "  print(\"Class-wise Recall: \",r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS9F01NH71Az"
      },
      "source": [
        "# 2. Implement the K-means (own code) clustering algorithm on the IRIS dataset(without using the label). Report the sum-square-error (SSE) by varying K value\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhJFLGnl8QX5"
      },
      "source": [
        "#importing necessary modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDW1uhL-EnCp",
        "outputId": "e939fe90-c29e-406e-bae6-bc697a6040f6"
      },
      "source": [
        "#loading the iris dataset\n",
        "iris = datasets.load_iris()      \n",
        "df = pd.DataFrame(iris.data)\n",
        "df.columns = iris.feature_names\n",
        "df['target'] = iris.target\n",
        "print(df)\n",
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "0                  5.1               3.5  ...               0.2       0\n",
            "1                  4.9               3.0  ...               0.2       0\n",
            "2                  4.7               3.2  ...               0.2       0\n",
            "3                  4.6               3.1  ...               0.2       0\n",
            "4                  5.0               3.6  ...               0.2       0\n",
            "..                 ...               ...  ...               ...     ...\n",
            "145                6.7               3.0  ...               2.3       2\n",
            "146                6.3               2.5  ...               1.9       2\n",
            "147                6.5               3.0  ...               2.0       2\n",
            "148                6.2               3.4  ...               2.3       2\n",
            "149                5.9               3.0  ...               1.8       2\n",
            "\n",
            "[150 rows x 5 columns]\n",
            "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "0                5.1               3.5  ...               0.2       0\n",
            "1                4.9               3.0  ...               0.2       0\n",
            "2                4.7               3.2  ...               0.2       0\n",
            "3                4.6               3.1  ...               0.2       0\n",
            "4                5.0               3.6  ...               0.2       0\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "     sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
            "145                6.7               3.0  ...               2.3       2\n",
            "146                6.3               2.5  ...               1.9       2\n",
            "147                6.5               3.0  ...               2.0       2\n",
            "148                6.2               3.4  ...               2.3       2\n",
            "149                5.9               3.0  ...               1.8       2\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJYT--ihGKV3",
        "outputId": "23f0694a-764f-40d3-a7a8-ca4369665592"
      },
      "source": [
        "#removed label column \n",
        "X = df.iloc[:,0:4].to_numpy()\n",
        "Y = df.iloc[:,-1].to_numpy() \n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvR2xXoUGK5F"
      },
      "source": [
        "#computes euclidean distance between x and y\n",
        "def euclidianDist( x, y):\n",
        "  return np.sqrt(np.sum(np.power((x - y), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFgdAI1HGLkt"
      },
      "source": [
        "# X: dataset without label column and it contains 4 features and 150 patterns\n",
        "# k: number of clusters\n",
        "# mx_itr : maximum no of iterations/ convergence criteria of kMeans  \n",
        "\n",
        "def kMeans(X,k,mx_itr=300):\n",
        "  centers = {}\n",
        "\n",
        "  #step 1: randomly choose k objects from dataset X as intial cluster centers\n",
        "  for i in range(k):\n",
        "    centers[i] = X[i]\n",
        "  \n",
        "  #step 4:check for convergence\n",
        "  for i in range(mx_itr):\n",
        "    curr_classification = {}   # stores the k clusters\n",
        "    distance = []       # stores distance between curr obj and k cluster centers\n",
        "    \n",
        "    for j in range(k):\n",
        "      curr_classification[j] = []  \n",
        "\n",
        "    #step 2: for each objects in X do some operations:\n",
        "    for row in X:\n",
        "\n",
        "      #1: compute distance between curr obj and k cluster centers\n",
        "      distance = [euclidianDist(row,centers[center]) for center in centers]\n",
        "\n",
        "      #2: assign the object to that cluster to which it is closest by finding the index of minimum distance cluster center\n",
        "      curr_class = distance.index(min(distance))\n",
        "      curr_classification[curr_class].append(row)\n",
        "\n",
        "    #step 3: update the cluster centers by taking the average of the patterns that belong to that cluster\n",
        "    for j in range(k):  \n",
        "      centers[j] = np.average(curr_classification[j],axis=0)  \n",
        "    \n",
        "  return centers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-upGDzUG-Tz"
      },
      "source": [
        "#classify each object to its closest cluster\n",
        "# predicts cluster of each objects : means objects belong to which cluster\n",
        "def predict(X,centers):\n",
        "  cls =[]\n",
        "  for row in X:\n",
        "    distance = [euclidianDist(row,centers[center]) for center in centers]\n",
        "    curr_class = distance.index(min(distance))\n",
        "    cls.append(curr_class)\n",
        "  return np.array(cls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI79K-arG-4V"
      },
      "source": [
        "# calculates SSE\n",
        "# x: dataset X\n",
        "# y: cluster no of each objects /objects belong to which cluster\n",
        "# center: centers of k clusters\n",
        "def distortion(x,y,center):\n",
        "  unique = np.unique(y)\n",
        "  classes = []\n",
        "  for i in range(len(unique)):\n",
        "    cls = x[y == unique[i]]\n",
        "    classes.append(cls)\n",
        "  \n",
        "  dist = 0\n",
        "  for i in range(len(unique)):\n",
        "    for j in range(len(classes[i])):\n",
        "      dist += (euclidianDist(classes[i][j],center[i])**2)\n",
        "             \n",
        "  return dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaLsJrltHJmN"
      },
      "source": [
        "squaredSum =[]  ## stores SSE of different k\n",
        "selfCenters = [] ## stores cluster centers of different k\n",
        "\n",
        "for k in range(1,10):\n",
        "    \n",
        "    center = kMeans(X,k)  #finding k cluster centers after k-means operation\n",
        "    predClass = predict(X,center)  #classify each object to its closest cluster\n",
        "    \n",
        "    sqSum = distortion(X,predClass,center) #calcullates SSE\n",
        "    squaredSum.append(sqSum)                                 \n",
        "    selfCenters.append(center)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "Knhm6fBzHmlN",
        "outputId": "95762b76-4da1-4f81-994d-2493fde8cfb2"
      },
      "source": [
        "#sum-square-error (SSE) by varying K(cluster count) value\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, 10), squaredSum)\n",
        "plt.ylabel('SSE')\n",
        "plt.xlabel('K')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NqqoHCx8RwO"
      },
      "source": [
        "# 3. Implement the MLP and k-means on the IRIS dataset using the inbuilt library function. Compare the results with the above-mentioned two questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-GwveDhwTNe"
      },
      "source": [
        "### Implement MLP Using Inbuilt library function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orHQ4roZ8Y7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94bf69c4-e21f-41c1-d3aa-0ad0034df9a6"
      },
      "source": [
        "#using inbuilt function\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#loading the data\n",
        "wine=datasets.load_wine()\n",
        "df=pd.DataFrame(data=np.c_[wine['data'],wine['target']],columns=wine['feature_names']+['target'])\n",
        "X_1=pd.DataFrame(wine.data)\n",
        "y_1=pd.DataFrame(wine.target)\n",
        "\n",
        "#normalize the dataset\n",
        "for column in X_1.columns:\n",
        "    X_1[column] = (X_1[column] - X_1[column].min()) / (X_1[column].max() - X_1[column].min()) \n",
        "\n",
        "X_1=X_1.to_numpy()\n",
        "y_1=y_1.to_numpy()\n",
        "#splitting the dataset\n",
        "\n",
        "# In the first step we will split the data in training and remaining dataset\n",
        "X_train_1, X_rem, y_train_1, y_rem = train_test_split(X_1,y_1, train_size=0.6)\n",
        "\n",
        "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
        "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
        "test_size = 0.5\n",
        "X_valid_1, X_test_1, y_valid_1, y_test_1 = train_test_split(X_rem,y_rem, test_size=0.5)\n",
        "clf = MLPClassifier(random_state=1, max_iter=700).fit(X_train_1, y_train_1.ravel())\n",
        "\n",
        "print(\"Accuracy: \",clf.score(X_test_1, y_test_1)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  91.66666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xd0ywA1fTOT",
        "outputId": "4f8b4a0a-276d-4392-bad9-c5482c43d711"
      },
      "source": [
        "k = 5\n",
        "kf = KFold(n_splits=k, random_state=None,shuffle=True) \n",
        "acc_score = []\n",
        "i=1\n",
        "for train_index , test_index in kf.split(X_1):\n",
        "    X_train_1 , X_test_1 = X_1[train_index],X_1[test_index]\n",
        "    y_train_1 , y_test_1 = y_1[train_index] , y_1[test_index]\n",
        "     \n",
        "    clf = MLPClassifier(random_state=1, max_iter=700).fit(X_train_1, y_train_1.ravel())\n",
        "    y_pred=clf.predict(X_test_1)\n",
        "    acc_score.append(clf.score(X_test_1, y_test_1)*100)\n",
        "    print(\"For \",i,\" fold: \\n\")\n",
        "    confusion(y_test_1,y_pred)\n",
        "    i=i+1 \n",
        "avg_acc_score = sum(acc_score)/k\n",
        "\n",
        "print('accuracy of each fold - {}'.format(acc_score))\n",
        "print('Avg accuracy : {}'.format(avg_acc_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For  1  fold: \n",
            "\n",
            "Class-wise Accuracy:  [1.         0.93333333 1.        ]\n",
            "Class-wise Precision:  [1.         1.         0.90909091]\n",
            "Class-wise Recall:  [1.         0.93333333 1.        ]\n",
            "For  2  fold: \n",
            "\n",
            "Class-wise Accuracy:  [1. 1. 1.]\n",
            "Class-wise Precision:  [1. 1. 1.]\n",
            "Class-wise Recall:  [1. 1. 1.]\n",
            "For  3  fold: \n",
            "\n",
            "Class-wise Accuracy:  [0.92857143 0.92857143 1.        ]\n",
            "Class-wise Precision:  [1.         0.92857143 0.88888889]\n",
            "Class-wise Recall:  [0.92857143 0.92857143 1.        ]\n",
            "For  4  fold: \n",
            "\n",
            "Class-wise Accuracy:  [1.    1.    0.875]\n",
            "Class-wise Precision:  [1.     0.9375 1.    ]\n",
            "Class-wise Recall:  [1.    1.    0.875]\n",
            "For  5  fold: \n",
            "\n",
            "Class-wise Accuracy:  [1. 1. 1.]\n",
            "Class-wise Precision:  [1. 1. 1.]\n",
            "Class-wise Recall:  [1. 1. 1.]\n",
            "accuracy of each fold - [97.22222222222221, 100.0, 94.44444444444444, 97.14285714285714, 100.0]\n",
            "Avg accuracy : 97.76190476190477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPFTuRawqjh"
      },
      "source": [
        "### Implement K-means Using Inbuilt library functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41siSr4nw5Gw"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "ssSklrn =[] #stores sse of different k\n",
        "centers = [] #stores centers of different k\n",
        "\n",
        "for k in range(1,10):   \n",
        "    km = KMeans(n_clusters=k)\n",
        "    km.fit(X)  #applying k-means\n",
        "    ssSklrn.append(km.inertia_)      #storing SSE                           \n",
        "    centers.append(km.cluster_centers_)  #storing centers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "sVfRyf3Aw-ku",
        "outputId": "e6bee5e4-c63a-4f8c-812a-4d64ebe70cee"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, 10), ssSklrn)\n",
        "plt.ylabel('SSE')\n",
        "plt.xlabel('K')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp_3M6-rWCWT"
      },
      "source": [
        "### compare Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmiBf-ZXWhNB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNy4rSvx8ZyK"
      },
      "source": [
        "# 4. Implement the PSO and genetic algorithm (GA) for clustering considering any one feature of the IRIS dataset and report SSE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EF4ipXTWVeT"
      },
      "source": [
        "### Implementation of GA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qecO7ZlQ8eO0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PyTBMzxWaXg"
      },
      "source": [
        "### Implementation of PSO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16l0ZjTwWdi-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}