{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cloud_lab9_Q3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "en39dUbt0RlZ"
      },
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tbm39CV0epb",
        "outputId": "5a7eb262-3d95-4081-b60b-716284157b04"
      },
      "source": [
        "!pip install pyspark==3.0.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.0\n",
            "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.7 MB 4.7 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=9cf3b56bdb6908fcddbad53bab492b021fd0add6ab85b3ee479f79d49be5e864\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c5/36/aef1bb711963a619063119cc032176106827a129c0be20e301\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O1cR-GF0pzd",
        "outputId": "0e4c91fd-ff22-42e2-bc92-2f67ba15e512"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.mllib.clustering import KMeans\n",
        "\n",
        "\n",
        "S3_DATA_SOURCE_PATH = \"/content/adultdata.txt\"\n",
        "s3_DATA_OUTPUT_PATH = \"/content/data-output\"\n",
        "if __name__ == \"__main__\":\n",
        "    sc = SparkContext(appName=\"KMeansExample\")\n",
        "\n",
        "    data = sc.textFile(S3_DATA_SOURCE_PATH)\n",
        "    parsedData = data.map(lambda line: np.array([x for x in line.split(', ')])[np.array([0,2,12])].astype(float))\n",
        "    \n",
        "    clusters = KMeans.train(parsedData, 2, maxIterations=20, initializationMode=\"random\")\n",
        "    cluster_center=clusters.centers\n",
        "    print(\"Centers:\",clusters.centers,file=sys.stdout)\n",
        "    \n",
        "    results = sc.parallelize(cluster_center)\n",
        "    def error(point):\n",
        "        center = clusters.centers[clusters.predict(point)]\n",
        "        return sqrt(sum([x**2 for x in (point - center)]))\n",
        "\n",
        "    WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "    print(\"Within Set Sum of Squared Error = \" + str(WSSSE),file=sys.stdout)\n",
        "\n",
        "    sc.stop()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Centers: [array([3.69546542e+01, 3.27532491e+05, 4.02616655e+01]), array([3.91434946e+01, 1.42207860e+05, 4.04981614e+01])]\n",
            "Within Set Sum of Squared Error = 1698914475.0772185\n"
          ]
        }
      ]
    }
  ]
}